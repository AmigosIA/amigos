LIST OF FUNCIONALITIES



Supervised Learning

1. Regression
   -Linear Regression

   ​	-Absolute/Squares

   ​	-Mean/Total Error

   -Multiple Dimensions

   -Polynomial

   -other

2. Perceptron

   -2 dimensions

   -n dimensions

   -other

3. Decision Tree

   -entropy

   -information gain (maximization)

   -Hyperparameters (maximum depth, minimum number of samples to split, min num of samples per leaf)

4. Naive Bayes

   -spam classifier

   -other

5. Support Vector Machines

   -maximum margin classifier

   ​	-classification err + margin err

   ​	-parameter C

   -inseparable classes classification

   -kernel methods (polynomial, rbf)

   -other

6. Ensemble

   https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
   https://scikit-learn.org/stable/modules/ensemble.html
   https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf
   https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf
   http://rob.schapire.net/papers/explaining-adaboost.pdf

   -bootstrap the data

   -subset the features

   -random forest

   -bagging

   -boosting (ada boost)

7. Metrics

   -confusion matrix

   -accuracy

   -precision

   -recall

   -f1 (and f-beta etc)

   -Receiver operating characteristic (ROC)

   -r2 score

   -entropy

   -information gain

8. training and tuning

   -model complexity graph

   -cross validation

   ​	-k-fold cross validation

   -training curves

   -grid search

9. Util

   -Regularization (L1, L2)

   -Feature Scaling (Standardizing, Normalizing)

   https://www.quora.com/Why-do-we-normalize-the-data

   -train_test_split

   -cross validation

   -CountVectorizer

DEEP LEARNING

1. Network

   -feed forward

   -backpropagation

   -activation functions (ReLU, Sigmoid, etc)

   -different error functions

2. Training methods
   
   -Early stopping
   
   -regularization
   
   -dropout
   
   -random restart
   
   -research: vanishing gradient problem
   
   -stochastic gradient descent
   
   -learning rate decay
   
   -momentum
   
   

*Check scikit-learn for reference*
